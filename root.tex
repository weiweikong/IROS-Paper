%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage[sort&compress]{natbib}
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed



\title{\LARGE \bf
Autonomous Landing of an UAV with a Ground-Based Actuated Infrared Stereo Vision System}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Weiwei Kong,\ Daibing Zhang,\ Xun Wang,\ Zhiwen Xian and Jianwei Zhang% <-this % stops a space
%\thanks{*This research is supported by National Natural Science Foundation of China and Innovation Cup funding in National University of Defense Technology.}% <-this % stops a space
\thanks{Weiwei Kong, Daibing Zhang, Xun Wang and Zhiwen Xian are with College of Mechatronic Engineering and Automation, National University of Defense Technology (NUDT), Changsha 410073, China. (Corresponding author: Weiwei Kong, e-mail:  {\tt\small kong@informatik.uni-hambrg.de}).
       }%
\thanks{Jianwei Zhang is with the Institute of Technical Aspects of Multimodal systems(TAMS), Department of Computer Science, University of Hamburg, Germany}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

In this study, we focus on the problem of landing an unmanned aerial vehicle (UAV) in unknown and GNSS-denied environments based on an infrared stereo vision system. This system is fixed on the ground and used to track the UAV's position during the landing process. In order to enlarge the search field of view (FOV), a pan-tilt unit (PTU) is employed to actuate the vision system. The infrared camera is chosen as the exteroceptive sensor for two main reasons: first, it can be used under all weather conditions and around the clock; second, infrared targets can be tracked based on infrared spectrum features at a lower computational cost compared to tracking texture features in visible spectrum. State-of-the-art active-contour based algorithms and the mean shift algorithm have been evaluated with regard to detecting and tracking an infrared target. Field experiments have been carried out using a Microdrone unmanned quadrotor and a fixed-wing unmanned aircraft, with both qualitative and quantitative evaluations. The results demonstrate that our system can track UAVs without artificial markers and is sufficient to enhance or even replace the GNSS-based localization in GNSS-denied environment or where its information is inaccurate.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

In the past decades, unmanned aircraft systems (UASs) have emerged in an increasing number of applications, mostly military but also civilian. Nowadays, safe autonomous motion control during the whole flight is essential for wide-spread acceptance of aircraft, where safely landing is the last, but not least crucial maneuver. Landing an unmanned aerial vehicle in autonomous fashion in unknown, GNSS-denied environments is still an open problem. The key challenges here are to control an unstable UAV and to localize the UAV using only information from vision sensors.

The goal of this research is to provide UAV motion control with an additional vision-based source of information, extracted by ground cameras. We provide an affirmative answer to the question of whether on-ground stereo vision systems can be used to sustain real-world GNSS-denied flight, by validating the aforementioned setup using through autonomous flight-tests. The main idea is to track the UAV during the landing process and estimate the relative position between the UAV and its landing point. Our landing system is consist of an infrared stereo camera and a PTU, which is shown in Fig. \ref{fig:system2}.
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/System22.eps}
      \caption{Infrared stereo system with PTU.}
      \label{fig:system2}
   \end{figure}

One of the great interests to the control community is vehicle navigation. According a recent survey \cite{Kendoul2012}, with regard to UAV, navigation can be defined as the process of data acquisition, data analysis, as well as extraction and inference of information about the vehicle's status and its surrounding environment, with the objective to accomplish assigned missions safely and successfully. The four core functions in a navigation system, from a lower to a higher level, are Sensing, State Estimation, Perception, and Situational Awareness.

Regarding this four functions, different types of sensors such as GNSS receivers, laser range finders (LRFs)\cite{Wu2013}\cite{Vasconcelos2010}, monocular cameras\cite{Wu2010}\cite{Weiss2011}, stereo cameras and the relatively new  RGB-D sensors\cite{Lange2012}\cite{Bachrach2012} have been explored. With respect to solving the landing problem, most researchers rely on on-board sensors\cite{Templeton2007}\cite{Wu2010}. On the other hand, ground-based systems\cite{PEBRIANTI2010}\cite{Abbeel2010}\cite{Wang2006} - particularly with infrared stereo vision - have not been considered for autonomous landing as often. Furthermore, the Sierra Nevada Corporation has developed the Tactical Automated Landing System (TALS) based on millimeter wavelength ground radar for all-weather autonomous landing \cite{TALS}. However, there are some disadvantages in aforementioned autonomous 
landing systems: (1) The range of available on-board sensors is restricted by the limited payload of UAVs. (2) For most of the field UAVs, the position and velocity estimation are based on GNSS. However, in some circumstances, such as urban or low altitude operations, the GNSS receiver antenna is prone to losing line-of-sight with satellites and making GNSS unable to deliver high quality position information \cite{RePEc:cdl:itsrrp:qt5tn6b3cp}, which is quite dangerous for closed-loop control systems in the landing maneuver. (3) Recent ground vision systems either are based on artificial markers \cite{Martnez2011a}, \cite{Martinez2009} or have limited ranges of observation\cite{Wang2006}.

Therefore, above mentioned considerations rule out employing such position sensors as GPS and places emphasis on other passive sensors. Besides, we do not employ radar (such as TALS), LRFs or other active range sensors, because we deliberately refrain from using the expensive custom hardware and the desire to avoid detection. In order to deal with these problems, we have built an infrared vision based ground landing system whose FOV is enlarged by a PTU, providing an alternative to exist systems.

The contributions of this work are three-fold: 
\begin{itemize}
\item First, a custom-built perception platform with a large FOV and can be employed round the clock under all-weather conditions.
\item Second, several state-of-the-art image processing algorithms have been evaluated and benchmarked against each other according to infrared targets detection and tracking.
\item Third, the whole system has been tested using field experiments, with a quadrotor and a fixed-wing aircraft.
\end{itemize}

The remainder of the paper is organized as follows: In the next section, we give a short overview of related works. Then in Section III, we describe the hardware and software of our landing system. The stereo infrared calibration method, target tracking algorithms and pose estimation for the aircraft is described in Section IV, followed the experimental results in Section V.


\section{RELATED RESEARCH}

In this section, we discuss some previous works related to autonomous vision based landing. In the last decade, vision based pose estimation has been extensively studied, such as in \cite{Shakernia1999}, \cite{Cesetti2010} and \cite{madison2008vision}. As  discussed in \cite{Kendoul2012}, except conventional IMU/GPS systems and state estimation using range sensors, vision-based state estimation systems can be classified into six main categories: {\it On-ground Vision} (OGV), {\it Visual Odometry} (VO), {\it Target Relative Navigation} (TGRN), {\it Terrain/landmark relative navigation} (TGR), {\it Concurrent Estimation of Motion and Structure} (SFM and SLAM) and {\it Bio-inspired Optic Flow Navigation} (BIOFN). Among the above methodologies, TGRN is one of the important assignments of vision based autonomous control. A potential application of it is autonomous, precise and safe landing.

An early autonomous navigation system for a model-scale helicopter (the Hummingbird) was reported in \cite{Conway:1995}. This system solely depended on GPS, which was traditionally favored as the primary navigation sensor. Furthermore, a number of significant achievements for UAV have been obtained based on fusing GPS and IMU \cite{Templeton2007}\cite{He2010}\cite{Scherer2008}. However, there are many situations where GPS is not reliable because the GPS signal can be lost due to multipath, satellites being occluded by building or even international jamming. To deal with these problems, vision based UAV control systems have been proposed. In \cite{bosse1997vision}, a vision-augmented navigation system for autonomous helicopter was presented where vision information is employed in the control loop. \cite{Amidi1999} proposed the visual odometer, which is a significant milestone for vision-based technique, able to provide accurate position and velocity. While various researchers have tested the visible light 
camera based system both indoor \cite{MATSUE2005}\cite{Bouabdallah2005} and outdoor \cite{Saripalli2003a}\cite{1331783}\cite{Saripalli2003}, their common drawback is that the computational complexity is too high under cluttered environments. In addition, since UAV is expected to be operated round the clock under all-weather conditions, IR camera is clearly a proper choice. Yakimenko\cite{Yakimenko2002} constituted  an on-board infrared vision system for UAV shipboard landing, where IR has  been found capable to simplify the relative pose estimation problem and reduce the susceptibility to glare. In \cite{Wenzel2010}, a micro infrared camera has been used to detect infrared spots on the ground to estimate the relative position. Such infrared systems, however, rely on artificial makers, hence not suited for general UAV application. IR is also employed in our system  but in a different manner, in order to eliminate the requirement of artificial markers.

One pivotal choice for vision systems is the number of cameras. Monocular camera has been utilized as a feedback sensor in \cite{538972}\cite{Shakernia1999}. \cite{Zhao2012}\cite{Johnson2005} used a downward-looking camera to detect the landing pad and search safe site in hazardous terrain. \cite{Bourquardez2007} presented a camera fixed on the front of an UAV to detect lines of the target runway. Unlike for binocular stereo vision, these algorithms have to not only detect significant features but also compute the motion between images which increases the burden of the on-board computer. In addition, stabilizing controllers based on monocular camera \cite{Zingg2010} are subject to drift over time. To eliminate the drift, Engel\cite{engel2012camera} presented an efficiency algorithm in which the camera is fused with other sensors such as GPS, IMU and pressure altimeter. However, there is an evident increase in system complexity. Stereo vision systems have also been employed in some early work \cite{
Saripalli2003a}\cite{1331783}\cite{Altug2005}, due to the fact that a stereo system can estimate depth
information in a single frame, and stereo could also be used to aid in outlier rejection for structure-from-motion algorithms. Then, another kind of camera set-up was presented in \cite{Martinez2009}, namely trinocular. This kind of system is composed by three or more cameras for extracting key features in order to obtain robust 3D position estimation. Further, another approach close to ours is  \cite{Wang2006}, which presented  a system using a step motor controlled web camera to recognize markers patched on the micro-aircraft. However, the weakness of this ground system is that the FOV is narrow. A more complicated system was shown in \cite{Altug2005} which contain two ground-based cameras and an on-board camera looking downwards.

Considering the dangerous nature of the UAV landing maneuver, system complexity is a reasonable price to pay for more robustness, so we presented a calibrated binocular infrared landing system to estimate relative position between UAV and landing site.


\section{SYSTEM ARCHITECTURE}
\subsection{Our Experimental platform}
Two kinds of UAV platforms have been employed to evaluate the vision system, namely a quadrotor and a fix-wing vehicle. The quadrotor platform is a commercial product from microdrones GmbH, with the type md4-200. This platform can fly by remote control or autonomously with the aid of our GPS waypoint navigation system. Additionally, the infrared features of four brushless motors are distinct compared to the background. The specification of md4-200 platform is detailed in Table \ref{tab:platform_specifications}, and Fig. \ref{fig:md4-200}.
   \begin{figure}[!tb]
      \centering
        \subfigure[]
	{
	  \label{fig:md4-200}
	  \includegraphics[height=2.5cm]{figs/md4-200.eps}
	}
        \subfigure[]
	{
	  \label{fig:fixed-wing}
	  \includegraphics[height=2.5cm]{figs/fixed-wing.eps}
	}
      \caption{(a) md4-200 quadrotor platform (b) fixed-wing platform}
   \end{figure}
   
  \begin{table}[!tb]
  \caption{The Technical Specifications of md4-200}
  \label{tab:platform_specifications}
  \begin{center}
  \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{ll}
    \hline
    {\textbf Items}  & \textbf{Description} \\
    \hline
    Vehicle mass & 800g \\
    Maximum Payload mass &	300g \\
    Diameter & 540mm from rotor shaft to rotor shaft \\
    Flight duration & up to 30 minutes \\
    Cruising speed & 8.0m/s \\
    Climb rate & 7.0m/s \\
    \hline
    \end{tabular}
  \end{center}
  \end{table}
The fixed-wing platform was selected with considerations of stable flight performance and landing in proper speed. Because the UAV is detected by the infrared sensor, a propeller should not be at the end of the body, so a puller-type medium size airplane was preferred as shown in Fig. \ref{fig:fixed-wing}, named Stormy Petrel. The length of Stormy Petrel is 1295 mm with 1575 mm wingspan and 5.7 kg weight. The maximum speed is 120 km/h, and the cruising speed is 90 km/h, equipped with ZENOAH G260PU-EI 26 CC motor.

Now we introduce the navigation module and autopilot module. iFLY-G2 (G2) is a small six-DOF (degree of freedom) navigation system, providing two combined navigation modes, i.e., GPS/INS and Attitude and Heading Reference System with Dead Reckoning(AHRS/DR). G2 includes a triaxial gyro, triaxial accelerometer, triaxial magnetic field meter, GPS module, barometric altimeter, airspeed gauge and thermometer. It provides real-time 3D information including attitude angle, angular rate, position, speed, acceleration, true air speed, calibrated air speed. The autopilot system is iFLY-F1A. It consists of the F1A autopilot system, a ground control station, a redundant power management module and an engine RPM monitoring module. iFLY-F1A is connected with iFLY-G2 through RS232, they are depicted in Fig. \ref{fig:iFly_G2} and Fig. \ref{fig:iFly_F1A}.
   \begin{figure}[!tb]
      \centering
        \subfigure[]
	{
	  \label{fig:iFly_G2}
	  \includegraphics[height=3cm]{figs/iFly_G2.eps}
	}
        \subfigure[]
	{
	  \label{fig:iFly_F1A}
	  \includegraphics[height=3cm]{figs/iFly_F1A.eps}
	}
      \caption{(a) iFLY-G2 Module  (b) iFLY-F1A Module}
   \end{figure}
\subsection{Stereo Vision System}
The stereo vision system is built on two infrared cameras, with model IRT301, which are produced by IRay Technology Co., Ltd. This kind of camera is an online surveillance infrared thermal imager. The detector materials is cooled MCT (HgCdTe) FPA and the sensor patch is 30 $\mathrm{\mu m}\times$30 $\mathrm{\mu m}$. The pixel resolution of the camera video is 320$\times$256 at 50 FPS. The focal length of the camera lens installed in the system is 22 mm with a Wide Field of View (WFOV) 24.6$\times$19.8 (in deg) , Medium Field Of View (MFOV) 4 $\times$3.2 and Narrow Field of View (NFOV) 0.92$\times$0.73. The spectral range is 3.7 $\mathrm{\mu m}\times$4.8 $\mathrm{\mu m}$. Generally, a man (1.8m$\times$0.5m) can be detected at the range 12km and recognized at 6km, which is sufficient for the landing process.
The PTU to actuate the stereo vision systems is PTS-3060 from PTS General Electronics Co., Ltd. PTS-3060 features internal wiring with slip-ring for 360-continuous pan with a tilt range from -30 to +90 deg. The range of pan speed is from 0.9 to 60 deg/second and the range of tilt range is from 0.9 to 40 deg/second. The assembled stereo vision system is illustrated in Fig. \ref{fig:system2}.

\subsection{Communication System}
The communication between the UAV and ground station is based on XTend RF Modems. This modem is a 900 MHz/1W device with up to 22km outdoor RF line-of-sight range. The interface data rate is form 10 bps to 230,000 bps. Under differential GPS (DGPS) navigation system, the DGPS information is transferred to UAV. Otherwise, the vision information is transferred. The spatial information is changed to 3D relative position estimated by the ground system. The complete experimental setup is shown in Fig. \ref{fig:system} and the overall architecture of the system is shown in Fig. \ref{fig:SystemStructure}.
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/System.eps}
      \caption{Ground stereo vision landing system.}
      \label{fig:system}
   \end{figure}
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/SystemStructure.eps}
      \caption{Architecture of the system.}
      \label{fig:SystemStructure}
   \end{figure}

\section{CALIBRATION and TRACKING}
In this section, we introduce the methods used for system calibration and target tracking with infrared characteristics.

\subsection{Infrared Camera Calibration}
Camera calibration is the process of estimating the {\it  Intrinsic Parameters} and {\it Extrinsic Parameters} of a stereo camera system. Generally, focal length, skew, distortion and image center are described by the camera's internal characteristics. On the other hand, extrinsic parameters deal with its position and orientation in the world. Infrared stereo camera calibration is similar to that of the visible camera because of the mechanical structure. In addition, for 3D computer vision, knowing intrinsic parameters is an essential step. To achieve the intrinsic parameters, chessboard pattern is recommended since it appears to produce accurate results. However, chessboard printed on the general paper cannot be distinctly captured by infrared sensors due to no apparent temperature difference between black squares and white squares. Without clearly intersection or corner features, calibration accuracy will decrease or even failed.

Thus, by pasting the black squares on the mirror, the new pattern was designed and heated by a heater before implementing calibration algorithm (see Fig.\ref{fig:Heater}). Then, the pattern with infrared feature can be obviously imaged by the infrared camera. One of the calibration image is shown in Fig.\ref{fig:Infrared_Pattern1}. With respect to long distance calibration, a 3 m$\times$3 m wood frame pattern was constructed. Each point of intersection was a heated incandescent lamp aim at enhancing the infrared features. The outcome of wood frame pattern in 50 m distance is shown in Fig.\ref{fig:Infrared_Pattern2}.
  \begin{figure}[!tb]
      \centering
      \subfigure[]
	{
      \includegraphics[height=2.5cm]{figs/Heater.eps}
      \label{fig:Heater}
      }
      \subfigure[]
	{
	\includegraphics[height=2.5cm]{figs/Infrared_Pattern1.eps}
	\label{fig:Infrared_Pattern1}
      }
      \caption{(a) Black squares pasted on mirror and heated by heater. \protect\\  \centerline{(b) Infrared features of the heated claibration board.}}
     \end{figure}
    \begin{figure}[!tb]
      \centering
      \includegraphics[height=3cm]{figs/Infrared_Pattern2.eps}
      \caption{Architecture of the system.}
      \label{fig:Infrared_Pattern2}
   \end{figure}

\subsection{Target Tracking Alogrithms}
In this section, we briefly review some of previous work on snakes or active contours based on ideas from curvature driven flows and the calculus of variations. 
Tracking is one of the basic control problems, in which we want the output to track or follow a standard signal and more importantly we attempt to make the tracking error as small as possible. In addition, the problem of visual tracking differs from standard tracking problem in the situation that the feedback signal is measured by imaging sensors. The information from imaging sensors has to be extracted via computer vision algorithms and interpreted by specific scheme before being calculated in the control loop. 
The complexity in this problem comes from the fact that there is no prior information of the relative position between UAV and ground system. Inevitably, the landing system must successfully identify and track UAV and support high accuracy relative position in a short period. Reliable evaluation of attitude, especially the height, will increase the safety of operation of unmanned aerial vehicle during the landing process. 



\subsubsection{Mean shift Method\cite{comaniciu2002mean}}
Mean shift method is one of the simple and fast visual tracking algorithms. This algorithm creates a confident map in the new frame mainly based on color histogram of the object in the previous image. The basic mean shift tracking algorithm is consist of five basic steps. Given ${q_u}$ of model and location \textbf{y} of target in previous fame: (1) Initialize location of target in current frame as \textbf{y}. (2) Compute $ {p_u(\textbf{y}) } $, $ u=1,\dots,m$ and $ \rho_u(\textbf{y},q) $. (3) Compute weight $ w_i,i=1,\dots,n_h$. (4) Apply mean shift: Compute new location \textbf{z} as
\begin{equation} 
\textbf{z}=\frac{\sum_{i=1}^{n_h} w_i g ( \left\| \frac{ \textbf{y} - \textbf{y}_i}{h}\right\|^2)\textbf{y}_i}{ \sum_{i=1}^{n_h} w_i g ( \left\| \frac{ \textbf{y} - \textbf{y}_i}{h}\right\|^2 )} 
\end{equation}                                                      
where $ g(x)=-k'(x)$ and $k(x)=\frac{1}{\sqrt{(2\pi)^d}}exp(-1/2 \left\| \textbf{x} \right\|^2)$ (5) Compute $ {p_u(\textbf{y}) },u=1,\dots,m $, and  $ \rho_u(\textbf{y},q) $. (6) While  $ \rho_u(\textbf{y},q) <   \rho_u(\textbf{z},q) $, do $ \textbf{z} \leftarrow \frac{1}{2}(\textbf{y}+\textbf{z})$. (7) if $ \left| \textbf{z} - \textbf{y} \right|$ is small engouth, stop. Else, set $ \textbf{y} \leftarrow \textbf{z}$ and goto Step 1.

\subsubsection{Snakes, Level Set Method and Fast Marching Method}
Some previous works, in \cite{betser2004automatic}\cite{sattigeri2007vision}, demonstrated that active contours are autonomous processes which employ image coherence in order to track various features of interest over time. A book \cite{blake1998active} and the references therein described snakes fit very spontaneously into a control framework. Additionally, they have been utilized in conjunction with Kalaman filtering. The kernel idea of the snakes is the viewpoint of energy. During the evolution of the contour, the splines are governed by an energy functional which defines the image dependent forces, internal forces, and certain constraints set by the user. The geometric active contour or snakes method is used to identify contours. The active contour method is an iterative method in which the calculus of variations is used to control the movement of a curve within the image. 
The starting point of Level Set Method is \cite{368173}\cite{Caselles1993} in which an active contour model founded on the level set formulation of the Euclidean curve shortening equation is proposed. Specifically, the model is:
\begin{equation} \label{eq:BasicModel}
\frac{\partial \Psi}{\partial t} = \phi(x,y) \left\|\nabla\Psi\right\| (\texttt{div}(\frac{\nabla \Psi}{\left\|\nabla\Psi\right\|})+\nu) 
\end{equation}
Here the function $ \phi(x,y) $ depends on the given image and is used as a ``stopping term''. Generally the term  $ \phi(x,y) $ is selected to be small near an intensity-based edge and acts to stop evolution when the contour gets close to an edge. According to \cite{368173}\cite{Caselles1993}, one may define $\phi(x,y) = \frac{1}{1 + \left\| \nabla G_{\sigma} \ast I \right\|^2 }$ where $I$  is the grey-scale intensity of pixel $x$, $y$ and $ G_{\sigma} $  is a Gaussian smoothing filter.  The function  $ \Psi(x,y,t)  $ evolves in equation (\ref{eq:BasicModel}) according to the associated level set flow for planar curve evolution in the normal direction with speed as a function of curvature which was discussed in \cite{sethian1985curvature}\cite{osher1988fronts}. 

On the other hand, the idea of length/area minimizing will modify the model in a precise manner. A curve could be explained as a one parameter $p$ , $C=(x(p),y(p))^T$, and we change the arc-length function as, $ds = (x_p^2 + x_p^2)^{1/2}$ to $ds_\phi = (x_p^2 + x_p^2)^{1/2} \phi dp$ where $\phi(x,y)$  is appositive differentiable function. Then the new metric $ ds_\phi$ is relative to corresponding gradient flow for shortening length.

The length function $L_\phi $  is defined as $ L_\phi = \int_{0}^{1}  \left\| \frac{\partial C}{\partial t}  \right\| \phi dp$. Then, by taking the first variation of the modified length function $L_\phi $  and using integration by parts as in, we get that 
\begin{equation}  L'_\phi(t) = - \int_{0}^{L_\phi(t)} \left\langle \frac{\partial C}{\partial t}, \phi \kappa \vec{N} - ( \nabla \phi \cdot \vec{N})\vec{N}   \right\rangle\end{equation}
where $ \kappa = \left\| C_{ss}  \right\| $  is the curvature, and $ \vec{N} = \frac{1}{\kappa}  C_{ss}$ denotes the unit normal to the curvature  $C$. The level set version of this is $ \frac{\partial \Psi}{\partial t}   =   \phi \left\|\nabla\Psi\right\|  \texttt{div}(\frac{\nabla \Psi}{\left\|\nabla\Psi\right\|})  + \nabla \phi \cdot \nabla \Psi$. This evolution should attract the contour very quickly to the feature which lies at the bottom of the potential well described by the gradient flow. As in \cite{sethian1985curvature}\cite{osher1988fronts}, in order to keep shrinking the contour, we may also add a constant inflation term, and so derive a modified model of (\ref{eq:BasicModel}) given by
\begin{equation}  \frac{\partial \Psi}{\partial t}   =   \phi \left\|\nabla\Psi\right\|  \texttt{div}(\frac{\nabla \Psi}{\left\|\nabla\Psi\right\|} + \nu)  + \nabla \phi \cdot \nabla \Psi\end{equation}
Then, a well-known {\it Eikonal equation} $\phi(x,y)\left\|T(x,y)\right\| = 1$  is built based on crossing time. Fast Marching Method can solve this Eikonal Equation through difference operator. \cite{johnson2007real} tested the Fast Marching Method to estimate the relative location between two UAVs.  
\subsubsection{Distance Regularized Level Set Evolution(DRLSE)\cite{Li2010}}
This method is a new type of level set evolution in which the regularity of the level set function is intrinsically maintained. Besides, using the relatively large time steps, iteration numbers and computation time of DRLSE are reduced, while ensures accurate computation and stable level set evolution.

   
\section{EXPERIMENTS}
We have conducted a series of field experiments to evaluate the stereo vision system. The experiments were carried out in different environments during both day and night. In the following, tracking experiments are detailed in Section V-A. Section V-B and Section V-C present the landing experiments for a quadrotor and a fixed-wing aircraft, respectively.

\subsection{Tracking Algorithms Experiments}
We test our system in five scenes: Scene I was selected to landing the quadrotor under a cloudy weather condition. This scene was selected to evaluate the algorithm's ability to manage the disturbance from cloud, which is usually a challenge mainly for infrared feature recognition. With regard to infrared feature varying due to distinct temperature, two experiments were carried out individually in a high temperature environment (Scene III) and a normal one (Scene IV). In addition, one trial was conducted in Scene IV, a foggy day in the morning, where the stereo system was fixed near the Huanghua airport, Changsha, China. At last, Scene V elaborated that a fixed-wing landing on a simulated carrier deck, more information will be detailed in Section V-C. 

The parameters in different algorithms have been tuned manually. For real flying testing (such as in Section V-B and Section V-C), the parameters were selected according to laboratory experiments, and if necessary, changing at the flying site. The results of mean shift, Snakes, Level Set, DRLSE, and Fast Marching are indicated by red, green, blue, pink and yellow contours in Fig. \ref{fig:Rslt_TrackAlgorithm} and each column presented one scene. Three typical frames of each scene have been selected to show the accuracy of the five different algorithms. The successful catching percentage in each scene is shown in Table \ref{tab:Five_Methods}.
   \begin{figure*}[!tb]
      \centering
      \includegraphics[width=\textwidth]{figs/Rslt_TrackAlgorithm.eps}
      \caption{The typical results of mean shift, Snakes, Level Set Method, DRLSE, Fast Marching Method are indicated by red, green, blue, pink and yellow contours.}
      \label{fig:Rslt_TrackAlgorithm}
   \end{figure*}
      \begin{table}[!tb]
    \caption{The Successful Detection Percentage of Five Different Methods}
    \begin{center}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    Scene  & Frames & Mean shift & Snakes & LSM  & DRLSE   & FMM     \\ \hline \hline
    1      &  389   & 87.4      & 97.7   & 98.2 & 100.0   & 100.0   \\ \hline
    2      &  1143  & 72.19     & 99.7   & 99.9 & 100.0   & 100.0   \\ \hline
    3      &  359   & 94.7      & 97.5   & 97.5 & 100.0   & 100.0   \\ \hline
    4      &  892   & 0.0       & 98.7   & 98.7 & 100.0   & 100.0   \\ \hline
    5      &  80    & 92.5      & 97.5   & 100.0& 100.0   & 100.0   \\ \hline
    \end{tabular}
    \end{center}

    \label{tab:Five_Methods}
    \end{table}
    
Mean shift can detect the target within clear background such as in Scene II, III and IV in a low percentage. However, it failed when various clouds in the air (Frame 205 in Scene I) or buildings have similar infrared feature histogram (Frame 046, 057 and 062 in Scene I). Snakes and Level Set Method achieved better results compared to mean shift, especially in Scene II, III and V. Yet, they also have low performance regarding to manage the disturbance of clouds (Frame 205 in Scene I). For target with complexity features (Frame 614 and 686 in Scene IV), Snakes and Level Set Method cannot catch the center of the target continuously, though only with high detecting percentage. Considering the accurateness of the final contour, DRLSE delicately depicted the edge of aircraft, no matter the target was far away (Frame 156 and 284 in Scene II) or just around the corner (Frame 297 in Scene III). In Scene II, under the high temperature environment, several miniature details were cached such in frame 284 in Scene II, 
both the landing gear and hook were strictly segmented from the background. However, the drawback of DRLSE is time consuming which is hardly to be used at real-time. More detailed time consuming analyses reference to \cite{Li2005}. Similarly, the Fast Marching Method also successfully caught the target in all five distinctive scenes. Though not all the features were correctly measured, Fast Marching Method indicated a contented cost of timing. The real time operating ability was also demonstrated in \cite{johnson2007real}. Therefore, given its efficiency and accuracy, Fast Marching Method is the proper algorithm for the stereo vision system.

\subsection{Quadrotor Aircraft Landing}
We have carried out field experiments during during both day night with similar weather condition (wind speed less than 4 m/s). The procedure of the experiment has been divided into two phases: (1) manually fly the quadrotor into 100 m height but out of the FOV (2) initial and activate the stereo vision system. When the tracking algorithms detect quadrotor successfully, the flight model will is changed from manually to autonomous. The ideal landing area was set 15 m in front of the landing system. A picture of the system configuration is shown in Fig.\ref{fig:Rslt_1_Evening_Exp}, which was taken during a night flight.
    \begin{figure}[!tb]
      \centering
        \subfigure[]
	{
	  \label{fig:Rslt_1_Evening_Exp}
	  \includegraphics[height=3cm]{figs/Rslt_1_Evening_Exp.eps}
	}
        \subfigure[]
	{
	  \label{fig:Rslt_2_quadrotor_result}
	  \includegraphics[height=3cm]{figs/Rslt_2_quadrotor_result.eps}
	}
	\caption{(a) Quadrotor down-toch point results. (b) Evening landing system setup.}
   \end{figure}
The center of the ideal landing point was set to (0, 0). The safe area was defined as a disc with 15 cm radius. Table \ref{tab:Quadrotor_Landing_Error} shows the statistical result of  touch-down point which is also visualized in Fig.\ref{fig:Rslt_2_quadrotor_result}. The above results demonstrated that the stereo vision systems can successfully guide the quadrotor with proper precision. The average error in x-axis is 8.1 cm and in y-axis is 6.5 cm. The maximum error and the standard deviation of the flight accuracy show uncritical oscillation around the idea landing point, considering the size of quadrotor(54 cm).
  \begin{table}[!tb]
  \caption{The results of touch-down point error in ten experiments}
  \label{tab:Quadrotor_Landing_Error}
  \begin{center}
  \begin{scriptsize}
  \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    No.  & 1  & 2 &  3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & $m$ & $\delta$  \\ \hline
    x      & 10 & 8 & -6 & 5 &-4 & 15& -9& 8 & 4 & 12 & 8.1     &3.57    \\ \hline
    y      & -5 & 9 &  4 &10 & 8 & -9&  5& 7 & -3&-5  & 6.5     &2.41    \\ \hline
    \end{tabular}
  \end{scriptsize}
  \end{center}
  \end{table}
  
\subsection{Fixed-wing Aircraft Landing}
For fixed-wing landing, the process can be divided into five segmentations: (1) Catching the aircraft by the vision system; (2) Starting the autonomous landing process. (3) Check the relative position and velocity of the aircraft (if not satisfied the safe landing threshold, transfer to missed approach maneuver). (4) Final approach process. (5) Land on the ground or hook the arresting cable. The whole process was shown in Fig.\ref{fig:Rslt_3_Landing_Result}.
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/Rslt_3_Landing_Result_2.eps}
      \caption{Fixed-wing landing process.}
      \label{fig:Rslt_3_Landing_Result}
   \end{figure}
Several experiments have been carried out with the fixed-wing aircraft. In December 2011, we counted our system in 1st International UAV Innovation Grand Prix, where a simulated carrier deck was constructed, and the landing field consists of a take-off runway, a flying zone, a drop zone and a landing area. The landing runway has a length of 60 m and width of 8 m. In the landing zone, there are four arresting cables which are 4 m away from each other. The cables were set to 30 mm above the landing ground. The goal was to land the UAV autonomously and hook the second arresting cable. The landing area is shown in Fig.\ref{fig:Rslt_4_DECK}.
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/Rslt_4_DECK.eps}
      \caption{Simulated Carrier Deck.}
      \label{fig:Rslt_4_DECK}
   \end{figure}
We set the standard landing point, the middle point of the second arresting cable, to (0, 0). Left deviation and forward deviation were set to positive. Besides, the relation between the safe area (a circle area of 1.5 m radius) and the five touch-down points is depicted in Fig.\ref{fig:Rslt_5_DeckLanding}. The safe area is shown in green and the arresting cables in blue. Only the fourth touch-down point was out of the general safe circle. In addition, for comparison, the data from GPS, IMU and Vision system have been recorded. The data from one flight is shown in Fig.\ref{fig:Rslt_IMU_Fusion}. Although the aircraft has not hooked the second arresting cable every time, the autonomous landings were all successful and the error are bounded to less than 2 m. 
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/Rslt_5_DeckLanding.eps}
      \caption{The result of fixed-wing touch-down point.}
      \label{fig:Rslt_5_DeckLanding}
   \end{figure}
   \begin{figure}[!tb]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/Rslt_IMU_Fusion.eps}
      \caption{The position comparation of GPS, IMU, EKT and camera.}
      \label{fig:Rslt_IMU_Fusion}
   \end{figure}
Some videos with eliminated drift demonstrating the robustness of our approach are publicly available: http://tams.informatik.uni-hamburg.de/videos/index.php.

\section{CONCLUSIONS}
In this research, we presented a novel infrared stereo vision system for UAV autonomous landing in GPS-denied field. Using infrared stereo cameras can reduce both the system cost and complexity of the tracking algorithm. Meanwhile, the search FOV is enlarged significantly by a PTU, in order to catch the aircraft as early as possible. The system has been evaluated according to robustness and precision by several field experiments with a quadrotor or a fixed-wing aircraft, both qualitatively and quantitatively. Although promising results have been obtained, there is still some open problems, such as low accuracy of fixed-wing touch-down points. The near future work is to improve the tracking algorithms and 3D position estimation methods. Besides, to attain higher aircraft status, visual navigation knowledge from the ground station should be properly fused with GPS and IMU.

%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}  % for using the IEEE conference template
\bibliography{root}  % your Reference Database Name instead of Reference
\end{document}
